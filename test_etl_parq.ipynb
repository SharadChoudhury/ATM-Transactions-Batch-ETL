{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1695299583691_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-86-240.ec2.internal:20888/proxy/application_1695299583691_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-86-240.ec2.internal:8042/node/containerlogs/container_1695299583691_0001_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=yarn appName=livy-session-0>"
     ]
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f796e18d1d0>"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# creating custom schema\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"month\", StringType(), True),\n",
    "    StructField(\"day\", IntegerType(), True),\n",
    "    StructField(\"weekday\", StringType(), True),\n",
    "    StructField(\"hour\", IntegerType(), True),\n",
    "    StructField(\"atm_status\", StringType(), True),\n",
    "    StructField(\"atm_id\", StringType(), True),\n",
    "    StructField(\"atm_manufacturer\", StringType(), True),\n",
    "    StructField(\"atm_location\", StringType(), True),\n",
    "    StructField(\"atm_streetname\", StringType(), True),\n",
    "    StructField(\"atm_street_number\", IntegerType(), True),\n",
    "    StructField(\"atm_zipcode\", IntegerType(), True),\n",
    "    StructField(\"atm_lat\", DoubleType(), True),\n",
    "    StructField(\"atm_lon\", DoubleType(), True),\n",
    "    StructField(\"currency\", StringType(), True),\n",
    "    StructField(\"card_type\", StringType(), True),\n",
    "    StructField(\"transaction_amount\", IntegerType(), True),\n",
    "    StructField(\"service\", StringType(), True),\n",
    "    StructField(\"message_code\", StringType(), True),\n",
    "    StructField(\"message_text\", StringType(), True),\n",
    "    StructField(\"weather_lat\", DoubleType(), True),\n",
    "    StructField(\"weather_lon\", DoubleType(), True),\n",
    "    StructField(\"weather_city_id\", IntegerType(), True),\n",
    "    StructField(\"weather_city_name\", StringType(), True),\n",
    "    StructField(\"temp\", DoubleType(), True),\n",
    "    StructField(\"pressure\", IntegerType(), True),\n",
    "    StructField(\"humidity\", IntegerType(), True),\n",
    "    StructField(\"wind_speed\", IntegerType(), True),\n",
    "    StructField(\"wind_deg\", IntegerType(), True),\n",
    "    StructField(\"rain_3h\", DoubleType(), True),\n",
    "    StructField(\"clouds_all\", IntegerType(), True),\n",
    "    StructField(\"weather_id\", IntegerType(), True),\n",
    "    StructField(\"weather_main\", StringType(), True),\n",
    "    StructField(\"weather_description\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+-------+----+----------+------+----------------+------------+-------------------+-----------------+-----------+-------+-------+--------+----------+------------------+----------+------------+------------+-----------+-----------+---------------+-----------------+-------+--------+--------+----------+--------+-------+----------+----------+------------+--------------------+\n",
      "|year|  month|day|weekday|hour|atm_status|atm_id|atm_manufacturer|atm_location|     atm_streetname|atm_street_number|atm_zipcode|atm_lat|atm_lon|currency| card_type|transaction_amount|   service|message_code|message_text|weather_lat|weather_lon|weather_city_id|weather_city_name|   temp|pressure|humidity|wind_speed|wind_deg|rain_3h|clouds_all|weather_id|weather_main| weather_description|\n",
      "+----+-------+---+-------+----+----------+------+----------------+------------+-------------------+-----------------+-----------+-------+-------+--------+----------+------------------+----------+------------+------------+-----------+-----------+---------------+-----------------+-------+--------+--------+----------+--------+-------+----------+----------+------------+--------------------+\n",
      "|\n",
      "|\n",
      "|\n",
      "|2017|January|  1| Sunday|   0|  Inactive|     3|             NCR|       Ikast|RÃƒÂ¥dhusstrÃƒÂ¦det|               12|       7430| 56.139|  9.154|     DKK|      VISA|              4166|Withdrawal|            |            |     56.139|      9.158|        2619426|            Ikast|281.150|    1011|     100|         6|     240|  0.000|        75|       300|     Drizzle|light intensity d...|\n",
      "|\n",
      "+----+-------+---+-------+----+----------+------+----------------+------------+-------------------+-----------------+-----------+-------+-------+--------+----------+------------------+----------+------------+------------+-----------+-----------+---------------+-----------------+-------+--------+--------+----------+--------+-------+----------+----------+------------+--------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "# reading the data from HDFS\n",
    "\n",
    "# df = spark.read \\\n",
    "#     .format(\"csv\") \\\n",
    "#     .option(\"header\", \"false\") \\\n",
    "#     .schema(schema) \\\n",
    "#     .load(\"data/part-m-00000\", sep='|')\n",
    "\n",
    "df = spark.read.load(\"ed0e1870-08c6-4d61-b0b4-54d66e5d723e.parquet\", inferSchema=True, header=False)\n",
    "\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|message_text                  |\n",
      "+------------------------------+\n",
      "|Suspected malfunction         |\n",
      "|                              |\n",
      "|\"Suspected malfunction        |\n",
      "|No response received from host|\n",
      "|\"Timed-out taking card        |\n",
      "|Timed-out taking money        |\n",
      "+------------------------------+"
     ]
    }
   ],
   "source": [
    "df.select('message_text').distinct().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2468572"
     ]
    }
   ],
   "source": [
    "# verify count of total records \n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2"
     ]
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()\n",
    "\n",
    "# by default 4 for csv , 2 for parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- weekday: string (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- atm_status: string (nullable = true)\n",
      " |-- atm_id: string (nullable = true)\n",
      " |-- atm_manufacturer: string (nullable = true)\n",
      " |-- atm_location: string (nullable = true)\n",
      " |-- atm_streetname: string (nullable = true)\n",
      " |-- atm_street_number: integer (nullable = true)\n",
      " |-- atm_zipcode: integer (nullable = true)\n",
      " |-- atm_lat: string (nullable = true)\n",
      " |-- atm_lon: string (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- card_type: string (nullable = true)\n",
      " |-- transaction_amount: integer (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- message_code: string (nullable = true)\n",
      " |-- message_text: string (nullable = true)\n",
      " |-- weather_lat: string (nullable = true)\n",
      " |-- weather_lon: string (nullable = true)\n",
      " |-- weather_city_id: integer (nullable = true)\n",
      " |-- weather_city_name: string (nullable = true)\n",
      " |-- temp: string (nullable = true)\n",
      " |-- pressure: integer (nullable = true)\n",
      " |-- humidity: integer (nullable = true)\n",
      " |-- wind_speed: integer (nullable = true)\n",
      " |-- wind_deg: integer (nullable = true)\n",
      " |-- rain_3h: string (nullable = true)\n",
      " |-- clouds_all: integer (nullable = true)\n",
      " |-- weather_id: integer (nullable = true)\n",
      " |-- weather_main: string (nullable = true)\n",
      " |-- weather_description: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "# check if the schema is correct\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate the count of null values in each column\n",
    "null_counts = df.select([sum(col(column).isNull().cast(\"int\")).alias(column) for column in df.columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(year=0, month=0, day=0, weekday=0, hour=0, atm_status=0, atm_id=0, atm_manufacturer=0, atm_location=0, atm_streetname=0, atm_street_number=0, atm_zipcode=0, atm_lat=0, atm_lon=0, currency=0, card_type=0, transaction_amount=0, service=0, message_code=0, message_text=0, weather_lat=0, weather_lon=0, weather_city_id=0, weather_city_name=0, temp=0, pressure=0, humidity=0, wind_speed=0, wind_deg=0, rain_3h=0, clouds_all=0, weather_id=0, weather_main=0, weather_description=0)]"
     ]
    }
   ],
   "source": [
    "null_counts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|message_code|\n",
      "+------------+\n",
      "|        4014|\n",
      "|        4018|\n",
      "|            |\n",
      "|        4006|\n",
      "|        4002|\n",
      "|        4019|\n",
      "|        0000|\n",
      "|        4017|\n",
      "+------------+"
     ]
    }
   ],
   "source": [
    "# analyze nulls in message_code column\n",
    "\n",
    "df.select(\"message_code\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|message_text                  |\n",
      "+------------------------------+\n",
      "|Suspected malfunction         |\n",
      "|                              |\n",
      "|\"Suspected malfunction        |\n",
      "|No response received from host|\n",
      "|\"Timed-out taking card        |\n",
      "|Timed-out taking money        |\n",
      "+------------------------------+"
     ]
    }
   ],
   "source": [
    "df.select(\"message_text\").distinct().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2459009"
     ]
    }
   ],
   "source": [
    "# Missing values as empty strings instaed of nulls for parquet files\n",
    "\n",
    "df.filter(df.message_code == \"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|message_code|\n",
      "+------------+\n",
      "|        null|\n",
      "|        4014|\n",
      "|        4018|\n",
      "|        4006|\n",
      "|        4002|\n",
      "|        4019|\n",
      "|        0000|\n",
      "|        4017|\n",
      "+------------+"
     ]
    }
   ],
   "source": [
    "# replace all empty strings with nulls\n",
    "\n",
    "df = df.replace(\"\",None)\n",
    "\n",
    "df.select('message_code').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|message_text                  |\n",
      "+------------------------------+\n",
      "|Suspected malfunction         |\n",
      "|null                          |\n",
      "|\"Suspected malfunction        |\n",
      "|No response received from host|\n",
      "|\"Timed-out taking card        |\n",
      "|Timed-out taking money        |\n",
      "+------------------------------+"
     ]
    }
   ],
   "source": [
    "df.select('message_text').distinct().show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df.replace(\"\\\"Suspected malfunction\", \"Suspected malfunction\", subset=['message_text'])\n",
    "df = df.replace(\"\\\"Timed-out taking card\", \"Timed-out taking card\", subset=['message_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|message_text                  |\n",
      "+------------------------------+\n",
      "|Suspected malfunction         |\n",
      "|null                          |\n",
      "|No response received from host|\n",
      "|Timed-out taking card         |\n",
      "|Timed-out taking money        |\n",
      "+------------------------------+"
     ]
    }
   ],
   "source": [
    "df.select('message_text').distinct().show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning Nulls in rain_3h column\n",
    "\n",
    "df.filter(df.rain_3h.isNull()).select('message_text').show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validating the value ranges in columns\n",
    "\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Location dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only distinct records in all dimension tables.\n",
    "df_location = df.select('atm_location','atm_streetname','atm_street_number','atm_zipcode','atm_lat','atm_lon').distinct()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_location.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_location.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_location.select('*').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a primary key 'location_id' based on row_number\n",
    "\n",
    "window_spec = Window.partitionBy().orderBy(df_location['atm_location'])\n",
    "df_location = df_location.select(row_number().over(window_spec).alias('location_id'), '*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_location.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_location.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_location.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating ATM dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also select the columns 'atm_location','atm_streetname','atm_street_number','atm_zipcode','atm_lat','atm_lon' to join\n",
    "# the table with location table using these columns to fetch the location_id\n",
    "\n",
    "df_atm = df.select('atm_id','atm_manufacturer','atm_location','atm_streetname',\n",
    "                   'atm_street_number', 'atm_zipcode','atm_lat','atm_lon').distinct()\n",
    "df_atm.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_atm.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating views for atm and location dimension to join them using sql and create foreign key location_id\n",
    "# in atm table based on location table primary key\n",
    "\n",
    "df_atm.createOrReplaceTempView('atm')\n",
    "df_location.createOrReplaceTempView('loc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining on atm_location to fetch the location_id from location dimension.\n",
    "# doing left join to fetch the location_id for each atm_id\n",
    "# joining is possible on atm_location as atm_location\n",
    "\n",
    "df_atm = spark.sql(\" select atm.atm_id, atm.atm_manufacturer, loc.location_id \\\n",
    "                     from atm left join loc on \\\n",
    "                     atm.atm_location = loc.atm_location and \\\n",
    "                     atm.atm_streetname = loc.atm_streetname and \\\n",
    "                     atm.atm_street_number = loc.atm_street_number and \\\n",
    "                     atm.atm_zipcode = loc.atm_zipcode and \\\n",
    "                     atm.atm_lat = loc.atm_lat and \\\n",
    "                     atm.atm_lon = loc.atm_lon \")\n",
    "\n",
    "df_atm.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_atm.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# atm_id in fact table should be atm_number as per the atm schema\n",
    "# and atm_id in atm schema should be the primary key generated using row_number\n",
    "\n",
    "window_spec = Window.partitionBy().orderBy(df_atm['atm_id'])\n",
    "df_atm = df_atm.select(row_number().over(window_spec).alias('atm_id'),\n",
    "                       col('atm_id').alias('atm_number'),\n",
    "                       'atm_manufacturer', 'location_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_atm.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_atm.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Date dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date = df.select(\"year\",\"month\",\"day\",\"hour\",\"weekday\").distinct()\n",
    "df_date.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using Spark version > 3.0, then use this for proper conversion to timestamp\n",
    "\n",
    "# spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the timestamp using year, month, day, hour columns\n",
    "# date_id is the primary key generated using row_number\n",
    "\n",
    "window_spec = Window.partitionBy().orderBy('year','month','day','hour')\n",
    "\n",
    "df_date = df_date.select(row_number().over(window_spec).alias('date_id'),\n",
    "                         to_timestamp(\n",
    "                            concat_ws(\n",
    "                                \" \",\n",
    "                                df[\"year\"].cast(\"string\"),  # Cast year to string\n",
    "                                df[\"month\"],\n",
    "                                df[\"day\"].cast(\"string\"),   # Cast day to string\n",
    "                                df[\"hour\"].cast(\"string\"),  # Cast hour to string\n",
    "                                    ),\n",
    "                    \"yyyy-MM-dd HH:mm:ss\" ).alias('full_date_time'),\n",
    "                         '*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Card type dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_card = df.select('card_type').distinct()\n",
    "df_card.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_card = df_card.select(row_number().over(Window.partitionBy().orderBy('card_type')).alias('card_type_id'), 'card_type')\n",
    "\n",
    "df_card.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_card.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_card.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating fact table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating all the views for all dimensions and fact table\n",
    "\n",
    "df_location.createOrReplaceTempView('loc')\n",
    "df_atm.createOrReplaceTempView('atm')\n",
    "df_date.createOrReplaceTempView('date')\n",
    "df_card.createOrReplaceTempView('card')\n",
    "df.createOrReplaceTempView('fact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining with card dimension\n",
    "\n",
    "df_fact = spark.sql(\"select card.card_type_id, fact.* \\\n",
    "                    from fact left join card on fact.card_type = card.card_type\")\n",
    "df_fact.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a view for the fact table based on updated columns after previous join\n",
    "\n",
    "df_fact.createOrReplaceTempView('fact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join with date dimension\n",
    "\n",
    "df_fact = spark.sql(\"select date.date_id, fact.* from fact left join date on \\\n",
    "                    fact.year = date.year and \\\n",
    "                    fact.month = date.month and \\\n",
    "                    fact.day = date.day and \\\n",
    "                    fact.hour = date.hour and \\\n",
    "                    fact.weekday = date.weekday \")\n",
    "\n",
    "df_fact.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact.createOrReplaceTempView('fact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join with location dimension\n",
    "\n",
    "df_fact = spark.sql(\"select loc.location_id weather_loc_id, fact.* \\\n",
    "                     from fact left join loc on \\\n",
    "                     fact.atm_location = loc.atm_location and \\\n",
    "                     fact.atm_streetname = loc.atm_streetname and \\\n",
    "                     fact.atm_street_number = loc.atm_street_number and \\\n",
    "                     fact.atm_zipcode = loc.atm_zipcode and \\\n",
    "                     fact.atm_lat = loc.atm_lat and \\\n",
    "                     fact.atm_lon = loc.atm_lon \")\n",
    "\n",
    "df_fact.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact.createOrReplaceTempView('fact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join with atm dimension\n",
    "# note that atm_id of fact will be mapped to atm_number of atm dimension\n",
    "\n",
    "df_fact = spark.sql(\"select atm.atm_id atm_id_dim, fact.* \\\n",
    "                    from fact left join atm on \\\n",
    "                    fact.atm_id = atm.atm_number and \\\n",
    "                    fact.atm_manufacturer = atm.atm_manufacturer and \\\n",
    "                    fact.weather_loc_id = atm.location_id \")\n",
    "\n",
    "df_fact.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact.createOrReplaceTempView('fact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting only the relevant columns in fact table\n",
    "\n",
    "df_fact = df_fact.select(row_number().over(Window.partitionBy().orderBy('date_id','atm_id_dim','weather_loc_id','card_type_id')).alias('trans_id'),\n",
    "                         col('atm_id_dim').alias('atm_id'),\n",
    "                         'weather_loc_id','date_id', 'card_type_id', 'atm_status', 'currency','service', 'transaction_amount',\n",
    "                         'message_code','message_text','rain_3h','clouds_all','weather_id','weather_main', 'weather_description'\n",
    "                         )\n",
    "\n",
    "df_fact.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact.createOrReplaceTempView('fact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing tables in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_atm_path = \"s3://atm-data-model/DIM_ATM/\" \n",
    "\n",
    "# Write atm dimension to S3 in CSV format\n",
    "df_atm.write \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(dim_atm_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_date_path = \"s3://atm-data-model/DIM_DATE/\" \n",
    "\n",
    "# Write date dimension to S3 in CSV format\n",
    "df_date.write \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(dim_date_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_card_path = \"s3://atm-data-model/DIM_CARD_TYPE/\" \n",
    "\n",
    "# Write card dimension to S3 in CSV format\n",
    "df_card.write \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(dim_card_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_location_path = \"s3://atm-data-model/DIM_LOCATION/\" \n",
    "\n",
    "# Write location dimension to S3 in CSV format\n",
    "df_location.write \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(dim_location_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_path = \"s3://atm-data-model/FACT_ATM_TRANS/\" \n",
    "\n",
    "# Write fact table to S3 in CSV format\n",
    "df_fact.write \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(fact_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
