Project Outline :

We have the ATM Transaction data hosted on a RDS instance within table SRC_ATM_TRANS in testdatabase: 
RDS Connection String -
jdbc:mysql://upgraddetest.cyaielc9bmnf.us-east-1.rds.amazonaws.com/testdatabase
	•	Username - student
	•	Password - STUDENT123
	•	Table Name - SRC_ATM_TRANS

To login to the RDS using mysql -h, we will need security group configuration between RDS and EC2. But Sqop will be
able toaccess the endpoint without any such configuration.

We have to ingest the data from RDS into HDFS in our EMR cluster and we will also perform 
transformations using Spark, So we have to create an EMR cluster with following services installed:
	Hadoop
	Sqoop
	Spark
	Hue
	Jupyter Notebook
	Livy
	Zeppelin


Then, in the EMR instance, setup Sqoop to connect to RDS : 
sudo -i 

wget https://de-mysql-connector.s3.amazonaws.com/mysql-connector-java-8.0.25.tar.gz
tar -xvf mysql-connector-java-8.0.25.tar.gz
cd mysql-connector-java-8.0.25/
sudo cp mysql-connector-java-8.0.25.jar /usr/lib/sqoop/lib/
exit 


Run the sqoop command (as hadoop user) to import data into HDFS: Decide the no of mappers by experimenting

# Storing as text format, it writes in /user/livy/data/part-m-0000
# If target-dir is not specified, it will write in /user/hadoop/<table_name>

sqoop import \
--connect jdbc:mysql://upgraddetest.cyaielc9bmnf.us-east-1.rds.amazonaws.com/testdatabase \
--table SRC_ATM_TRANS \
--username student --password STUDENT123 \
--target-dir /user/livy/data \
--fields-terminated-by '|' \
--lines-terminated-by '\n' \
-m 1

In the above method, the Sqoop job gets executed faster but the resulting file size is very large : 506 MB

Note: Sqoop only creates a directory with same name as table if no target-dir is specified, or the target-dir lies in /user/hadoop/
Otherwise, it directly writes in the specified target-dir.


# Optional:  Saving as parquet files to optimize performance :

sqoop import \
--connect jdbc:mysql://upgraddetest.cyaielc9bmnf.us-east-1.rds.amazonaws.com/testdatabase \
--table SRC_ATM_TRANS \
--username student --password STUDENT123 \
--null-string '\\N' --null-non-string '\\N' \
--target-dir /user/livy/ \
--fields-terminated-by '|' \
-m 1 \
--as-parquetfile

- While writing as parquet file, the sqoop job takes a little more time but it is more suited big data workloads, because : 
- Parquet files store schema information, making it easier to handle schema evolution in Spark. 
- It creates a directory .metadata to store the schema information.
- Parquet files are typically compressed more effectively than CSV files, which results in reduced storage costs and faster I/O.
The parquet file that was written was only of size 42.6 MB, which is significantly less than normal text files.

But, in this method, it is not possible to set custom schema (as per Spark 2.4), so I used the text file for spark transformation code.
First read the parquet file with inferSchema=true, then cast the columns to appropriate data types as required.


Check if the data is imported correctly : 
hadoop fs -ls /user/livy/data




# Spark task
Run the SparkETLCode.ipynb notebook in Jupyter to create the fact and dimensions and store them into separate
files in S3.



# Redshift task
Create a redshift cluster with two nodes of dc2.large instances.
Create the schema and tables. Then load data into these tables from s3 bucket. 
Follow the commands in model_creation.sql
(S3 objects URI and region can be noted from their properties tab)

Now analyse the data using the queries from analysis.sql
