Project Outline :

We have the ATM Transaction data hosted on a RDS instance within table SRC_ATM_TRANS in testdatabase: 
RDS Connection String -
jdbc:mysql://upgraddetest.cyaielc9bmnf.us-east-1.rds.amazonaws.com/testdatabase
	•	Username - student
	•	Password - STUDENT123
	•	Table Name - SRC_ATM_TRANS

To login to the RDS using mysql -h, we will need security group configuration between RDS and EC2. But Sqop will be
able toaccess the endpoint without any such configuration.

We have to ingest the data from RDS into HDFS in our EMR cluster and we will also perform 
transformations using Spark, So we have to create an EMR cluster with following services installed:
	Hadoop
	Sqoop
	Spark
	Hue
	Jupyter Notebook
	Livy


Then, in the EMR instance, setup Sqoop to connect to RDS : 
sudo -i 

wget https://de-mysql-connector.s3.amazonaws.com/mysql-connector-java-8.0.25.tar.gz
tar -xvf mysql-connector-java-8.0.25.tar.gz
cd mysql-connector-java-8.0.25/
sudo cp mysql-connector-java-8.0.25.jar /usr/lib/sqoop/lib/
exit 


Run the sqoop command (as hadoop user) to import data into HDFS: Decide the no of mappers by experimenting

# Storing as text format, it writes in /user/livy/data/part-m-0000
# If target-dir is not specified, it will write in /user/hadoop/<table_name>

sqoop import \
--connect jdbc:mysql://upgraddetest.cyaielc9bmnf.us-east-1.rds.amazonaws.com/testdatabase \
--table SRC_ATM_TRANS \
--username student --password STUDENT123 \
--target-dir /user/livy/data \
--fields-terminated-by '|' \
-m 1

In the above method, the Sqoop job gets executed faster but the resulting file size is very large : 506 MB

Note: Sqoop only creates a directory with same name as table if no target-dir is specified, or the target-dir lies in /user/hadoop/
Otherwise, it directly writes in the specified target-dir.




# Optional:  Saving as parquet files to optimize performance :

sqoop import \
--connect jdbc:mysql://upgraddetest.cyaielc9bmnf.us-east-1.rds.amazonaws.com/testdatabase \
--table SRC_ATM_TRANS \
--username student --password STUDENT123 \
--target-dir /user/livy/ \
--fields-terminated-by '|' \
-m 1 \
--as-parquetfile

While writing as parquet file, the sqoop job takes a little more time but it is more suited big data workloads, because : 
Parquet files store schema information, making it easier to handle schema evolution in Spark. 
It creates a directory .metadata to store the schema information.
Parquet files are typically compressed more effectively than CSV files, which results in reduced storage costs and faster I/O.
The parquet file that was written was only of size 42.6 MB, which is significantly less than normal text files.

But, in this method, it is not possible to set custom schema (as per Spark 2.4), so I used the text file for spark transformation code.
Here, you will have to cast the different columns to specific data types. 


Check if the data is imported correctly : 
hadoop fs -ls /user/livy/data

Rename the file:
hdfs dfs -mv /user/livy/data/part-m* /user/livy/ATM.csv

# Spark task
Run the SparkETLCode.ipynb notebook in Jupyter to create the fact and dimensions and store them into separate
files in S3.


# Redshift task

